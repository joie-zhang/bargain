adversary_model,Tier,Elo,Source,Reasoning,N,Mean Util,Std Util,Min Util,Max Util,Mean Share,Consensus %,Avg Rounds
gemini-3-pro,Strong,1490,Closed,False,22,69.45,18.07,38.0,100.0,0.47,1.0,1.95
gemini-3-flash,Strong,1472,Closed,False,22,67.95,21.66,19.0,100.0,0.45,1.0,1.95
claude-opus-4-5-thinking-32k,Strong,1470,Closed,True,22,68.59,17.93,33.0,100.0,0.46,1.0,1.59
claude-opus-4-5,Strong,1467,Closed,False,22,68.59,22.53,21.0,100.0,0.45,1.0,1.91
claude-sonnet-4-5,Strong,1450,Closed,False,22,69.77,20.85,13.0,100.0,0.48,1.0,1.95
glm-4.7,Strong,1441,Open,False,21,75.71,19.42,34.0,100.0,0.5,1.0,1.24
gpt-5.2-high,Strong,1436,Closed,True,22,73.27,20.64,18.0,100.0,0.49,1.0,1.55
qwen3-max,Strong,1434,Open,False,22,69.27,17.89,40.0,100.0,0.49,1.0,1.05
deepseek-r1-0528,Strong,1418,Open,True,21,75.86,17.19,42.0,100.0,0.51,1.0,1.19
grok-4,Strong,1409,Closed,False,19,72.37,15.76,49.0,100.0,0.47,1.0,1.63
claude-haiku-4-5,Medium,1403,Closed,False,22,69.0,18.94,20.0,100.0,0.49,1.0,2.73
deepseek-r1,Medium,1397,Open,True,20,78.6,14.86,54.0,100.0,0.51,1.0,1.15
claude-sonnet-4,Medium,1390,Closed,False,22,71.82,23.73,16.0,100.0,0.48,1.0,1.59
claude-3.5-sonnet,Medium,1373,Closed,False,22,72.95,19.17,33.0,100.0,0.49,1.0,1.73
gemma-3-27b-it,Medium,1365,Open,False,22,68.23,19.83,31.0,100.0,0.51,1.0,3.27
o3-mini-high,Medium,1364,Closed,True,22,71.5,19.04,22.0,100.0,0.48,1.0,1.86
deepseek-v3,Medium,1358,Open,False,22,69.05,22.67,24.0,100.0,0.47,1.0,1.64
gpt-4o,Medium,1346,Closed,False,22,64.95,18.98,41.0,100.0,0.48,1.0,1.86
QwQ-32B,Medium,1336,Open,False,22,74.59,16.1,50.0,100.0,0.54,1.0,1.59
llama-3.3-70b-instruct,Medium,1320,Open,False,22,63.14,19.49,37.0,100.0,0.45,1.0,2.5
Qwen2.5-72B-Instruct,Medium,1303,Open,False,22,63.18,26.44,9.0,100.0,0.44,1.0,2.0
gemma-2-27b-it,Medium,1288,Open,False,21,66.48,21.34,9.0,100.0,0.45,1.0,3.95
Meta-Llama-3-70B-Instruct,Medium,1277,Open,False,20,66.1,22.76,16.0,100.0,0.46,1.0,3.1
claude-3-haiku,Medium,1262,Closed,False,17,48.94,26.03,17.0,100.0,0.42,1.0,6.47
phi-4,Medium,1256,Open,False,21,68.38,23.91,22.0,100.0,0.49,1.0,2.67
amazon-nova-micro,Weak,1241,Closed,False,15,80.07,11.79,67.0,100.0,0.52,1.0,1.13
llama-3.1-8b-instruct,Weak,1212,Open,False,7,45.14,9.41,33.0,60.0,0.49,1.0,6.14
Llama-3.2-3B-Instruct,Weak,1167,Open,False,22,68.45,14.85,45.0,100.0,0.5,1.0,2.82
Mistral-7B-Instruct-v0.2,Weak,1151,Open,False,4,68.0,34.5,32.0,99.0,0.55,1.0,5.0
Phi-3-mini-128k-instruct,Weak,1130,Open,False,22,65.77,17.89,26.0,99.0,0.53,1.0,1.41
Llama-3.2-1B-Instruct,Weak,1112,Open,False,21,72.24,15.96,49.0,100.0,0.54,1.0,2.19
