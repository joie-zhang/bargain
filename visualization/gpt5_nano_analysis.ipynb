{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-5-nano Negotiation Analysis\n",
    "\n",
    "This notebook analyzes negotiation experiments where GPT-5-nano is paired against various models across different tiers.\n",
    "\n",
    "## Model Tiers\n",
    "- **STRONG TIER** (Elo >= 1415): gemini-3-pro, gpt-5.2-high, claude-opus-4-5, kimi-k2-thinking, deepseek-r1-0528, qwen3-235b-a22b-instruct-2507\n",
    "- **MEDIUM TIER** (1290 <= Elo < 1415): claude-4-5-haiku, o4-mini-2025-04-16, gpt-oss-20b\n",
    "- **WEAK TIER** (Elo < 1290): llama-3.3-70b-instruct, llama-3.1-8b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tier definitions with Elo ratings\n",
    "MODEL_INFO = {\n",
    "    # Strong tier (Elo >= 1415)\n",
    "    \"gemini-3-pro\": {\"tier\": \"Strong\", \"elo\": 1492, \"source\": \"Closed\", \"reasoning\": True},\n",
    "    \"gpt-5.2-high\": {\"tier\": \"Strong\", \"elo\": 1465, \"source\": \"Closed\", \"reasoning\": True},\n",
    "    \"claude-opus-4-5\": {\"tier\": \"Strong\", \"elo\": 1462, \"source\": \"Closed\", \"reasoning\": False},\n",
    "    \"kimi-k2-thinking\": {\"tier\": \"Strong\", \"elo\": 1438, \"source\": \"Open\", \"reasoning\": True},\n",
    "    \"deepseek-r1-0528\": {\"tier\": \"Strong\", \"elo\": 1426, \"source\": \"Open\", \"reasoning\": True},\n",
    "    \"qwen3-235b-a22b-instruct-2507\": {\"tier\": \"Strong\", \"elo\": 1418, \"source\": \"Open\", \"reasoning\": False},\n",
    "    \n",
    "    # Medium tier (1290 <= Elo < 1415)\n",
    "    \"claude-4-5-haiku\": {\"tier\": \"Medium\", \"elo\": 1378, \"source\": \"Closed\", \"reasoning\": False},\n",
    "    \"o4-mini-2025-04-16\": {\"tier\": \"Medium\", \"elo\": 1362, \"source\": \"Closed\", \"reasoning\": True},\n",
    "    \"gpt-oss-20b\": {\"tier\": \"Medium\", \"elo\": 1315, \"source\": \"Open\", \"reasoning\": False},\n",
    "    \n",
    "    # Weak tier (Elo < 1290)\n",
    "    \"llama-3.3-70b-instruct\": {\"tier\": \"Weak\", \"elo\": 1276, \"source\": \"Open\", \"reasoning\": False},\n",
    "    \"llama-3.1-8b-instruct\": {\"tier\": \"Weak\", \"elo\": 1193, \"source\": \"Open\", \"reasoning\": False},\n",
    "}\n",
    "\n",
    "TIER_ORDER = [\"Strong\", \"Medium\", \"Weak\"]\n",
    "TIER_COLORS = {\"Strong\": \"#e74c3c\", \"Medium\": \"#f39c12\", \"Weak\": \"#27ae60\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment directories to search\n",
    "RESULTS_DIR = Path(\"/scratch/gpfs/DANQIC/jz4391/bargain/experiments/results\")\n",
    "\n",
    "SCALING_EXPERIMENTS = [\n",
    "    \"scaling_experiment_20260117_185910\",  # Most recent\n",
    "    \"scaling_experiment_20260117_172734\",\n",
    "    \"scaling_experiment_20260116_052234\",\n",
    "    \"scaling_experiment_old_20260116_052119\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_result(result_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"Load a single experiment result file.\"\"\"\n",
    "    try:\n",
    "        with open(result_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_successful_experiment(result: Dict) -> bool:\n",
    "    \"\"\"Check if an experiment completed successfully.\n",
    "    \n",
    "    Filters out:\n",
    "    - Experiments that didn't reach consensus (optional - we may want to include these)\n",
    "    - Experiments with missing/invalid data\n",
    "    - API failures (usually indicated by empty or malformed results)\n",
    "    \"\"\"\n",
    "    if result is None:\n",
    "        return False\n",
    "    \n",
    "    # Must have key fields\n",
    "    required_fields = ['final_utilities', 'final_round', 'config', 'agent_preferences']\n",
    "    for field in required_fields:\n",
    "        if field not in result:\n",
    "            return False\n",
    "    \n",
    "    # Must have valid utilities (not None or empty)\n",
    "    utilities = result.get('final_utilities', {})\n",
    "    if not utilities or len(utilities) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Utilities should be numeric\n",
    "    for agent, util in utilities.items():\n",
    "        if util is None or not isinstance(util, (int, float)):\n",
    "            return False\n",
    "    \n",
    "    # Must have valid preferences\n",
    "    prefs = result.get('agent_preferences', {})\n",
    "    if not prefs or len(prefs) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def parse_model_pair_from_path(path: Path) -> Tuple[str, str]:\n",
    "    \"\"\"Extract model pair from experiment path.\n",
    "    \n",
    "    Path format: .../gpt-5-nano_vs_gemini-3-pro/weak_first/comp_0.5/run_1/\n",
    "    Returns: (gpt-5-nano, gemini-3-pro)\n",
    "    \"\"\"\n",
    "    parts = path.parts\n",
    "    for part in parts:\n",
    "        if '_vs_' in part:\n",
    "            models = part.split('_vs_')\n",
    "            if len(models) == 2:\n",
    "                return tuple(models)\n",
    "    return (None, None)\n",
    "\n",
    "\n",
    "def parse_experiment_config_from_path(path: Path) -> Dict:\n",
    "    \"\"\"Extract experiment configuration from path.\n",
    "    \n",
    "    Returns: {model_order, competition_level, run_number, scaling_experiment}\n",
    "    \"\"\"\n",
    "    parts = path.parts\n",
    "    config = {\n",
    "        'model_order': None,\n",
    "        'competition_level': None,\n",
    "        'run_number': None,\n",
    "        'scaling_experiment': None\n",
    "    }\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if part.startswith('scaling_experiment'):\n",
    "            config['scaling_experiment'] = part\n",
    "        elif part in ['weak_first', 'strong_first']:\n",
    "            config['model_order'] = part\n",
    "        elif part.startswith('comp_'):\n",
    "            try:\n",
    "                config['competition_level'] = float(part.replace('comp_', ''))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif part.startswith('run_'):\n",
    "            try:\n",
    "                config['run_number'] = int(part.replace('run_', ''))\n",
    "            except ValueError:\n",
    "                pass\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_gpt5_nano_experiments() -> List[Dict]:\n",
    "    \"\"\"Discover all GPT-5-nano experiments across scaling experiment directories.\"\"\"\n",
    "    all_experiments = []\n",
    "    \n",
    "    for scaling_exp in SCALING_EXPERIMENTS:\n",
    "        exp_dir = RESULTS_DIR / scaling_exp\n",
    "        if not exp_dir.exists():\n",
    "            print(f\"Skipping missing directory: {scaling_exp}\")\n",
    "            continue\n",
    "        \n",
    "        # Find all gpt-5-nano model pair directories\n",
    "        for model_pair_dir in exp_dir.glob(\"gpt-5-nano_vs_*\"):\n",
    "            if not model_pair_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            weak_model, strong_model = parse_model_pair_from_path(model_pair_dir)\n",
    "            if weak_model != 'gpt-5-nano':\n",
    "                continue\n",
    "            \n",
    "            # Find all experiment result files\n",
    "            for result_file in model_pair_dir.glob(\"**/experiment_results.json\"):\n",
    "                result = load_experiment_result(result_file)\n",
    "                \n",
    "                if not is_successful_experiment(result):\n",
    "                    continue\n",
    "                \n",
    "                path_config = parse_experiment_config_from_path(result_file)\n",
    "                \n",
    "                experiment_data = {\n",
    "                    'file_path': str(result_file),\n",
    "                    'weak_model': weak_model,\n",
    "                    'strong_model': strong_model,\n",
    "                    'opponent_model': strong_model,  # Since weak_model is always gpt-5-nano\n",
    "                    **path_config,\n",
    "                    **result\n",
    "                }\n",
    "                \n",
    "                all_experiments.append(experiment_data)\n",
    "    \n",
    "    return all_experiments\n",
    "\n",
    "\n",
    "# Load all experiments\n",
    "print(\"Discovering GPT-5-nano experiments...\")\n",
    "experiments = discover_gpt5_nano_experiments()\n",
    "print(f\"Found {len(experiments)} successful experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "def extract_experiment_features(exp: Dict) -> Dict:\n",
    "    \"\"\"Extract key features from experiment data for DataFrame.\"\"\"\n",
    "    utilities = exp.get('final_utilities', {})\n",
    "    strategic = exp.get('strategic_behaviors', {})\n",
    "    \n",
    "    # Determine which agent is GPT-5-nano based on model_order\n",
    "    model_order = exp.get('model_order', 'weak_first')\n",
    "    if model_order == 'weak_first':\n",
    "        nano_agent = 'Agent_Alpha'\n",
    "        opponent_agent = 'Agent_Beta'\n",
    "    else:\n",
    "        nano_agent = 'Agent_Beta'\n",
    "        opponent_agent = 'Agent_Alpha'\n",
    "    \n",
    "    nano_utility = utilities.get(nano_agent, 0)\n",
    "    opponent_utility = utilities.get(opponent_agent, 0)\n",
    "    \n",
    "    opponent_model = exp.get('opponent_model', 'unknown')\n",
    "    opponent_info = MODEL_INFO.get(opponent_model, {'tier': 'Unknown', 'elo': 0})\n",
    "    \n",
    "    return {\n",
    "        'opponent_model': opponent_model,\n",
    "        'opponent_tier': opponent_info.get('tier', 'Unknown'),\n",
    "        'opponent_elo': opponent_info.get('elo', 0),\n",
    "        'opponent_source': opponent_info.get('source', 'Unknown'),\n",
    "        'opponent_reasoning': opponent_info.get('reasoning', False),\n",
    "        'model_order': model_order,\n",
    "        'competition_level': exp.get('competition_level', 0),\n",
    "        'consensus_reached': exp.get('consensus_reached', False),\n",
    "        'final_round': exp.get('final_round', 10),\n",
    "        'nano_utility': nano_utility,\n",
    "        'opponent_utility': opponent_utility,\n",
    "        'total_utility': nano_utility + opponent_utility,\n",
    "        'utility_share': nano_utility / (nano_utility + opponent_utility) if (nano_utility + opponent_utility) > 0 else 0.5,\n",
    "        'manipulation_attempts': strategic.get('manipulation_attempts', 0),\n",
    "        'anger_expressions': strategic.get('anger_expressions', 0),\n",
    "        'gaslighting_attempts': strategic.get('gaslighting_attempts', 0),\n",
    "        'cooperation_signals': strategic.get('cooperation_signals', 0),\n",
    "        'scaling_experiment': exp.get('scaling_experiment', ''),\n",
    "        'run_number': exp.get('run_number', 0),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([extract_experiment_features(exp) for exp in experiments])\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"GPT-5-NANO EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal successful experiments: {len(df)}\")\n",
    "print(f\"\\nExperiments by opponent model:\")\n",
    "print(df['opponent_model'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nExperiments by opponent tier:\")\n",
    "print(df['opponent_tier'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nExperiments by competition level:\")\n",
    "print(df['competition_level'].value_counts().sort_index().to_string())\n",
    "\n",
    "print(f\"\\nExperiments by model order:\")\n",
    "print(df['model_order'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage matrix: which model pairs have data at which competition levels\n",
    "coverage = df.pivot_table(\n",
    "    index='opponent_model',\n",
    "    columns='competition_level',\n",
    "    values='nano_utility',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Add tier info\n",
    "coverage['tier'] = coverage.index.map(lambda x: MODEL_INFO.get(x, {}).get('tier', 'Unknown'))\n",
    "coverage = coverage.sort_values(['tier', 'opponent_model'])\n",
    "\n",
    "print(\"\\nData Coverage Matrix (count of experiments):\")\n",
    "print(coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate stats by opponent model\n",
    "agg_by_model = df.groupby('opponent_model').agg({\n",
    "    'nano_utility': ['mean', 'std', 'count'],\n",
    "    'opponent_utility': ['mean', 'std'],\n",
    "    'utility_share': ['mean', 'std'],\n",
    "    'consensus_reached': 'mean',\n",
    "    'final_round': 'mean',\n",
    "    'manipulation_attempts': 'mean',\n",
    "    'gaslighting_attempts': 'mean',\n",
    "    'cooperation_signals': 'mean',\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "agg_by_model.columns = ['_'.join(col).strip() for col in agg_by_model.columns.values]\n",
    "\n",
    "# Add tier and elo\n",
    "agg_by_model['tier'] = agg_by_model.index.map(lambda x: MODEL_INFO.get(x, {}).get('tier', 'Unknown'))\n",
    "agg_by_model['elo'] = agg_by_model.index.map(lambda x: MODEL_INFO.get(x, {}).get('elo', 0))\n",
    "\n",
    "# Sort by tier and elo\n",
    "tier_order = {'Strong': 0, 'Medium': 1, 'Weak': 2, 'Unknown': 3}\n",
    "agg_by_model['tier_order'] = agg_by_model['tier'].map(tier_order)\n",
    "agg_by_model = agg_by_model.sort_values(['tier_order', 'elo'], ascending=[True, False])\n",
    "agg_by_model = agg_by_model.drop('tier_order', axis=1)\n",
    "\n",
    "print(\"\\nAggregate Statistics by Opponent Model:\")\n",
    "print(agg_by_model.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate stats by tier\n",
    "agg_by_tier = df.groupby('opponent_tier').agg({\n",
    "    'nano_utility': ['mean', 'std', 'count'],\n",
    "    'opponent_utility': ['mean', 'std'],\n",
    "    'utility_share': ['mean', 'std'],\n",
    "    'consensus_reached': 'mean',\n",
    "    'final_round': 'mean',\n",
    "}).round(2)\n",
    "\n",
    "agg_by_tier.columns = ['_'.join(col).strip() for col in agg_by_tier.columns.values]\n",
    "agg_by_tier = agg_by_tier.reindex(['Strong', 'Medium', 'Weak'])\n",
    "\n",
    "print(\"\\nAggregate Statistics by Opponent Tier:\")\n",
    "print(agg_by_tier.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats by competition level\n",
    "agg_by_comp = df.groupby('competition_level').agg({\n",
    "    'nano_utility': ['mean', 'std', 'count'],\n",
    "    'utility_share': ['mean', 'std'],\n",
    "    'consensus_reached': 'mean',\n",
    "    'final_round': 'mean',\n",
    "    'total_utility': 'mean',\n",
    "}).round(2)\n",
    "\n",
    "agg_by_comp.columns = ['_'.join(col).strip() for col in agg_by_comp.columns.values]\n",
    "\n",
    "print(\"\\nAggregate Statistics by Competition Level:\")\n",
    "print(agg_by_comp.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: GPT-5-nano utility by opponent model (sorted by Elo)\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Sort by elo\n",
    "model_order = df.groupby('opponent_model')['opponent_elo'].first().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Create box plot\n",
    "plot_df = df[df['opponent_model'].isin(model_order)].copy()\n",
    "plot_df['opponent_model'] = pd.Categorical(plot_df['opponent_model'], categories=model_order, ordered=True)\n",
    "\n",
    "colors = [TIER_COLORS.get(MODEL_INFO.get(m, {}).get('tier', 'Unknown'), '#95a5a6') for m in model_order]\n",
    "\n",
    "bp = sns.boxplot(data=plot_df, x='opponent_model', y='nano_utility', ax=ax, palette=colors)\n",
    "sns.stripplot(data=plot_df, x='opponent_model', y='nano_utility', ax=ax, \n",
    "              color='black', alpha=0.3, size=4)\n",
    "\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.7, label='Equal Split')\n",
    "ax.set_xlabel('Opponent Model (sorted by Elo, high to low)', fontsize=12)\n",
    "ax.set_ylabel('GPT-5-nano Utility', fontsize=12)\n",
    "ax.set_title('GPT-5-nano Utility Distribution by Opponent Model', fontsize=14)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add tier legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=TIER_COLORS[t], label=f'{t} Tier') for t in TIER_ORDER]\n",
    "legend_elements.append(plt.Line2D([0], [0], color='gray', linestyle='--', label='Equal Split'))\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_utility_by_opponent.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Utility share by opponent tier\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Box plot of utility share by tier\n",
    "tier_df = df[df['opponent_tier'].isin(TIER_ORDER)].copy()\n",
    "tier_df['opponent_tier'] = pd.Categorical(tier_df['opponent_tier'], categories=TIER_ORDER, ordered=True)\n",
    "\n",
    "sns.boxplot(data=tier_df, x='opponent_tier', y='utility_share', ax=axes[0],\n",
    "            palette=[TIER_COLORS[t] for t in TIER_ORDER])\n",
    "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "axes[0].set_xlabel('Opponent Tier', fontsize=12)\n",
    "axes[0].set_ylabel('GPT-5-nano Utility Share', fontsize=12)\n",
    "axes[0].set_title('Utility Share by Opponent Tier', fontsize=14)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Right: Bar plot of mean utility share with error bars\n",
    "tier_means = tier_df.groupby('opponent_tier')['utility_share'].agg(['mean', 'std', 'count'])\n",
    "tier_means = tier_means.reindex(TIER_ORDER)\n",
    "tier_means['se'] = tier_means['std'] / np.sqrt(tier_means['count'])\n",
    "\n",
    "bars = axes[1].bar(range(len(TIER_ORDER)), tier_means['mean'], \n",
    "                   yerr=tier_means['se'] * 1.96,  # 95% CI\n",
    "                   color=[TIER_COLORS[t] for t in TIER_ORDER],\n",
    "                   capsize=5, edgecolor='black', linewidth=1.5)\n",
    "axes[1].axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "axes[1].set_xticks(range(len(TIER_ORDER)))\n",
    "axes[1].set_xticklabels(TIER_ORDER)\n",
    "axes[1].set_xlabel('Opponent Tier', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Utility Share (95% CI)', fontsize=12)\n",
    "axes[1].set_title('GPT-5-nano Mean Utility Share by Opponent Tier', fontsize=14)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Add count annotations\n",
    "for i, (tier, row) in enumerate(tier_means.iterrows()):\n",
    "    axes[1].annotate(f'n={int(row[\"count\"])}', xy=(i, row['mean'] + row['se']*1.96 + 0.03),\n",
    "                     ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_utility_share_by_tier.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Utility vs Elo scatter with regression\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Scatter plot colored by tier\n",
    "for tier in TIER_ORDER:\n",
    "    tier_data = df[df['opponent_tier'] == tier]\n",
    "    ax.scatter(tier_data['opponent_elo'], tier_data['nano_utility'], \n",
    "               c=TIER_COLORS[tier], alpha=0.6, s=60, label=f'{tier} Tier', edgecolors='white')\n",
    "\n",
    "# Add regression line\n",
    "valid_df = df[df['opponent_elo'] > 0]\n",
    "if len(valid_df) > 5:\n",
    "    z = np.polyfit(valid_df['opponent_elo'], valid_df['nano_utility'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(valid_df['opponent_elo'].min(), valid_df['opponent_elo'].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'k--', alpha=0.7, linewidth=2, label=f'Trend (slope={z[0]:.3f})')\n",
    "\n",
    "ax.axhline(y=50, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Opponent Model Elo Rating', fontsize=12)\n",
    "ax.set_ylabel('GPT-5-nano Utility', fontsize=12)\n",
    "ax.set_title('GPT-5-nano Utility vs Opponent Elo Rating', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_utility_vs_elo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print correlation\n",
    "corr = valid_df['opponent_elo'].corr(valid_df['nano_utility'])\n",
    "print(f\"\\nCorrelation between opponent Elo and GPT-5-nano utility: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Heatmap of utility by opponent model and competition level\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Pivot table for heatmap\n",
    "heatmap_data = df.pivot_table(\n",
    "    index='opponent_model',\n",
    "    columns='competition_level',\n",
    "    values='nano_utility',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Sort by elo\n",
    "elo_order = df.groupby('opponent_model')['opponent_elo'].first().sort_values(ascending=False).index\n",
    "heatmap_data = heatmap_data.reindex(elo_order)\n",
    "\n",
    "# Add tier labels to index\n",
    "new_index = [f\"{m} ({MODEL_INFO.get(m, {}).get('tier', '?')[0]})\" for m in heatmap_data.index]\n",
    "heatmap_data.index = new_index\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', center=50,\n",
    "            cbar_kws={'label': 'GPT-5-nano Mean Utility'}, ax=ax)\n",
    "ax.set_xlabel('Competition Level', fontsize=12)\n",
    "ax.set_ylabel('Opponent Model (S=Strong, M=Medium, W=Weak)', fontsize=12)\n",
    "ax.set_title('GPT-5-nano Utility Heatmap\\n(Models sorted by Elo, high to low)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_utility_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: Competition level effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Utility by competition level\n",
    "comp_levels = sorted(df['competition_level'].unique())\n",
    "colors = plt.cm.coolwarm(np.linspace(0, 1, len(comp_levels)))\n",
    "\n",
    "for i, comp in enumerate(comp_levels):\n",
    "    comp_data = df[df['competition_level'] == comp]\n",
    "    axes[0].scatter([comp] * len(comp_data), comp_data['nano_utility'], \n",
    "                    c=[colors[i]], alpha=0.5, s=50)\n",
    "\n",
    "# Add mean line\n",
    "comp_means = df.groupby('competition_level')['nano_utility'].mean()\n",
    "axes[0].plot(comp_means.index, comp_means.values, 'ko-', linewidth=2, markersize=10, label='Mean')\n",
    "axes[0].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Competition Level', fontsize=12)\n",
    "axes[0].set_ylabel('GPT-5-nano Utility', fontsize=12)\n",
    "axes[0].set_title('Utility vs Competition Level', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Total utility (efficiency) by competition level\n",
    "total_means = df.groupby('competition_level')['total_utility'].mean()\n",
    "total_std = df.groupby('competition_level')['total_utility'].std()\n",
    "\n",
    "axes[1].bar(total_means.index, total_means.values, width=0.15, \n",
    "            yerr=total_std.values, capsize=5, color='steelblue', edgecolor='black')\n",
    "axes[1].axhline(y=200, color='green', linestyle='--', alpha=0.7, label='Max possible (200)')\n",
    "axes[1].set_xlabel('Competition Level', fontsize=12)\n",
    "axes[1].set_ylabel('Total Utility (Both Agents)', fontsize=12)\n",
    "axes[1].set_title('Negotiation Efficiency by Competition Level', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_competition_effects.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 6: Rounds to consensus\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Histogram of final rounds\n",
    "consensus_df = df[df['consensus_reached'] == True]\n",
    "axes[0].hist(consensus_df['final_round'], bins=range(1, 12), edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(x=consensus_df['final_round'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {consensus_df[\"final_round\"].mean():.1f}')\n",
    "axes[0].set_xlabel('Final Round', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Rounds to Consensus', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Rounds by opponent tier\n",
    "tier_rounds = consensus_df.groupby('opponent_tier')['final_round'].agg(['mean', 'std', 'count'])\n",
    "tier_rounds = tier_rounds.reindex(TIER_ORDER)\n",
    "tier_rounds['se'] = tier_rounds['std'] / np.sqrt(tier_rounds['count'])\n",
    "\n",
    "bars = axes[1].bar(range(len(TIER_ORDER)), tier_rounds['mean'],\n",
    "                   yerr=tier_rounds['se'] * 1.96,\n",
    "                   color=[TIER_COLORS[t] for t in TIER_ORDER],\n",
    "                   capsize=5, edgecolor='black')\n",
    "axes[1].set_xticks(range(len(TIER_ORDER)))\n",
    "axes[1].set_xticklabels(TIER_ORDER)\n",
    "axes[1].set_xlabel('Opponent Tier', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Rounds to Consensus', fontsize=12)\n",
    "axes[1].set_title('Negotiation Length by Opponent Tier', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_rounds_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 7: Strategic behavior comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "behaviors = ['manipulation_attempts', 'gaslighting_attempts', 'anger_expressions', 'cooperation_signals']\n",
    "titles = ['Manipulation Attempts', 'Gaslighting Attempts', 'Anger Expressions', 'Cooperation Signals']\n",
    "\n",
    "for ax, behavior, title in zip(axes.flat, behaviors, titles):\n",
    "    # Mean by opponent model\n",
    "    model_means = df.groupby('opponent_model')[behavior].mean().sort_values(ascending=False)\n",
    "    colors = [TIER_COLORS.get(MODEL_INFO.get(m, {}).get('tier', 'Unknown'), '#95a5a6') for m in model_means.index]\n",
    "    \n",
    "    bars = ax.barh(range(len(model_means)), model_means.values, color=colors, edgecolor='black')\n",
    "    ax.set_yticks(range(len(model_means)))\n",
    "    ax.set_yticklabels(model_means.index)\n",
    "    ax.set_xlabel(f'Mean {title}')\n",
    "    ax.set_title(title)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [Patch(facecolor=TIER_COLORS[t], label=f'{t} Tier') for t in TIER_ORDER]\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.99, 0.99))\n",
    "\n",
    "plt.suptitle('Strategic Behaviors by Opponent Model', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_strategic_behaviors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 8: Model order effects (weak_first vs strong_first)\n",
    "if 'model_order' in df.columns and df['model_order'].nunique() > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Left: Utility by model order\n",
    "    order_df = df[df['model_order'].isin(['weak_first', 'strong_first'])]\n",
    "    \n",
    "    sns.boxplot(data=order_df, x='model_order', y='nano_utility', ax=axes[0],\n",
    "                palette=['#3498db', '#e74c3c'])\n",
    "    axes[0].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_xlabel('Model Order', fontsize=12)\n",
    "    axes[0].set_ylabel('GPT-5-nano Utility', fontsize=12)\n",
    "    axes[0].set_title('Utility by Speaking Order', fontsize=14)\n",
    "    \n",
    "    # Right: Utility by model order and tier\n",
    "    order_tier_means = order_df.groupby(['model_order', 'opponent_tier'])['nano_utility'].mean().unstack()\n",
    "    order_tier_means = order_tier_means[TIER_ORDER] if all(t in order_tier_means.columns for t in TIER_ORDER) else order_tier_means\n",
    "    \n",
    "    x = np.arange(len(TIER_ORDER))\n",
    "    width = 0.35\n",
    "    \n",
    "    if 'weak_first' in order_tier_means.index:\n",
    "        axes[1].bar(x - width/2, order_tier_means.loc['weak_first'], width, label='Weak First', color='#3498db')\n",
    "    if 'strong_first' in order_tier_means.index:\n",
    "        axes[1].bar(x + width/2, order_tier_means.loc['strong_first'], width, label='Strong First', color='#e74c3c')\n",
    "    \n",
    "    axes[1].axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(TIER_ORDER)\n",
    "    axes[1].set_xlabel('Opponent Tier', fontsize=12)\n",
    "    axes[1].set_ylabel('Mean GPT-5-nano Utility', fontsize=12)\n",
    "    axes[1].set_title('Order Effect by Opponent Tier', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/gpt5_nano_order_effects.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient model order variation for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 9: Reasoning vs Non-reasoning models\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "reasoning_df = df[df['opponent_reasoning'].notna()].copy()\n",
    "reasoning_df['reasoning_label'] = reasoning_df['opponent_reasoning'].map({True: 'Reasoning', False: 'Non-Reasoning'})\n",
    "\n",
    "sns.boxplot(data=reasoning_df, x='reasoning_label', y='nano_utility', ax=ax,\n",
    "            palette=['#9b59b6', '#1abc9c'])\n",
    "sns.stripplot(data=reasoning_df, x='reasoning_label', y='nano_utility', ax=ax,\n",
    "              color='black', alpha=0.3, size=4)\n",
    "\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Opponent Model Type', fontsize=12)\n",
    "ax.set_ylabel('GPT-5-nano Utility', fontsize=12)\n",
    "ax.set_title('GPT-5-nano Performance vs Reasoning/Non-Reasoning Models', fontsize=14)\n",
    "\n",
    "# Add stats\n",
    "for i, reason_type in enumerate(['Reasoning', 'Non-Reasoning']):\n",
    "    type_data = reasoning_df[reasoning_df['reasoning_label'] == reason_type]['nano_utility']\n",
    "    ax.annotate(f'n={len(type_data)}\\nmean={type_data.mean():.1f}', \n",
    "                xy=(i, type_data.max() + 5), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/gpt5_nano_vs_reasoning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary table\n",
    "summary_table = df.groupby('opponent_model').agg({\n",
    "    'nano_utility': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'utility_share': ['mean', 'std'],\n",
    "    'consensus_reached': ['mean'],\n",
    "    'final_round': ['mean'],\n",
    "}).round(2)\n",
    "\n",
    "summary_table.columns = ['Mean Util', 'Std Util', 'Min Util', 'Max Util', 'N',\n",
    "                          'Mean Share', 'Std Share', 'Consensus %', 'Avg Rounds']\n",
    "\n",
    "# Add model info\n",
    "summary_table['Tier'] = summary_table.index.map(lambda x: MODEL_INFO.get(x, {}).get('tier', 'Unknown'))\n",
    "summary_table['Elo'] = summary_table.index.map(lambda x: MODEL_INFO.get(x, {}).get('elo', 0))\n",
    "summary_table['Source'] = summary_table.index.map(lambda x: MODEL_INFO.get(x, {}).get('source', 'Unknown'))\n",
    "summary_table['Reasoning'] = summary_table.index.map(lambda x: MODEL_INFO.get(x, {}).get('reasoning', False))\n",
    "\n",
    "# Reorder columns\n",
    "summary_table = summary_table[['Tier', 'Elo', 'Source', 'Reasoning', 'N', \n",
    "                                'Mean Util', 'Std Util', 'Min Util', 'Max Util',\n",
    "                                'Mean Share', 'Consensus %', 'Avg Rounds']]\n",
    "\n",
    "# Sort by tier and elo\n",
    "tier_order = {'Strong': 0, 'Medium': 1, 'Weak': 2, 'Unknown': 3}\n",
    "summary_table['tier_order'] = summary_table['Tier'].map(tier_order)\n",
    "summary_table = summary_table.sort_values(['tier_order', 'Elo'], ascending=[True, False])\n",
    "summary_table = summary_table.drop('tier_order', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE SUMMARY: GPT-5-nano vs All Opponents\")\n",
    "print(\"=\"*100)\n",
    "print(summary_table.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "summary_table.to_csv('figures/gpt5_nano_summary.csv')\n",
    "print(\"\\nSaved to figures/gpt5_nano_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key findings summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overall_mean = df['nano_utility'].mean()\n",
    "overall_share = df['utility_share'].mean()\n",
    "print(f\"\\n1. Overall GPT-5-nano Performance:\")\n",
    "print(f\"   - Mean utility: {overall_mean:.1f}\")\n",
    "print(f\"   - Mean utility share: {overall_share:.1%}\")\n",
    "print(f\"   - Total experiments: {len(df)}\")\n",
    "\n",
    "print(f\"\\n2. Performance by Opponent Tier:\")\n",
    "for tier in TIER_ORDER:\n",
    "    tier_data = df[df['opponent_tier'] == tier]\n",
    "    if len(tier_data) > 0:\n",
    "        print(f\"   - vs {tier}: {tier_data['nano_utility'].mean():.1f} utility (n={len(tier_data)})\")\n",
    "\n",
    "# Best and worst matchups\n",
    "model_perf = df.groupby('opponent_model')['nano_utility'].mean().sort_values()\n",
    "print(f\"\\n3. Best matchups for GPT-5-nano:\")\n",
    "for model in model_perf.tail(3).index:\n",
    "    print(f\"   - vs {model}: {model_perf[model]:.1f} utility\")\n",
    "\n",
    "print(f\"\\n4. Hardest matchups for GPT-5-nano:\")\n",
    "for model in model_perf.head(3).index:\n",
    "    print(f\"   - vs {model}: {model_perf[model]:.1f} utility\")\n",
    "\n",
    "# Correlation with Elo\n",
    "valid_df = df[df['opponent_elo'] > 0]\n",
    "if len(valid_df) > 5:\n",
    "    corr = valid_df['opponent_elo'].corr(valid_df['nano_utility'])\n",
    "    print(f\"\\n5. Correlation with opponent Elo: {corr:.3f}\")\n",
    "    if corr < -0.1:\n",
    "        print(\"   -> GPT-5-nano performs worse against stronger models\")\n",
    "    elif corr > 0.1:\n",
    "        print(\"   -> GPT-5-nano performs better against stronger models (unexpected!)\")\n",
    "    else:\n",
    "        print(\"   -> No strong relationship between opponent strength and performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full dataframe for further analysis\n",
    "df.to_csv('figures/gpt5_nano_full_data.csv', index=False)\n",
    "print(f\"Full dataset saved to figures/gpt5_nano_full_data.csv ({len(df)} rows)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
