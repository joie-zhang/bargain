{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug: Agent-Model API Mismatch Bug\n",
    "\n",
    "**CRITICAL FINDING**: This is NOT just a prompt assignment issue - the actual API calls are going to the WRONG model!\n",
    "\n",
    "## Evidence from `strong_first/budget_20000/comp_1_0`\n",
    "\n",
    "**experiment_results.json (agent_performance) shows CORRECT assignment:**\n",
    "```json\n",
    "\"Agent_Alpha\": {\"model\": \"claude-opus-4-5-thinking-32k\"}\n",
    "\"Agent_Beta\": {\"model\": \"gpt-5-nano\"}\n",
    "```\n",
    "\n",
    "**But token_usage patterns in all_interactions.json show SWAPPED API calls:**\n",
    "- `Agent_Alpha`: `{\"total_tokens\": 1153}` - NO reasoning_tokens → **GPT-5 pattern**\n",
    "- `Agent_Beta`: `{\"input_tokens\": ..., \"reasoning_tokens\": 3904}` → **Claude pattern**\n",
    "\n",
    "## Bug Summary\n",
    "\n",
    "1. The agent **objects** have the correct model names (verified via `agent.model_name`)\n",
    "2. But the actual **API calls** are going to the wrong endpoints\n",
    "3. This affects BOTH `weak_first` and `strong_first` experiments\n",
    "\n",
    "## Key Files to Investigate\n",
    "1. `strong_models_experiment/agents/agent_factory.py` - Creates agents (lines 55-76)\n",
    "2. `strong_models_experiment/experiment.py` - Manages agent order (lines 131-147, 207-218)\n",
    "3. `negotiation/llm_agents.py` - Actual API calls\n",
    "4. `scripts/generate_ttc_configs.sh` - SLURM script model ordering (lines 436-440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models list from config generator: ['claude-opus-4-5-thinking-32k', 'gpt-5-nano']\n",
      "  models[0] = claude-opus-4-5-thinking-32k (this becomes Agent_Alpha)\n",
      "  models[1] = gpt-5-nano (this becomes Agent_Beta)\n"
     ]
    }
   ],
   "source": [
    "# Simulate the exact flow that happens in the experiment\n",
    "\n",
    "# Step 1: Config generator (generate_ttc_experiments.py line 191)\n",
    "# Always passes: '--models', reasoning_model, baseline_model\n",
    "reasoning_model = \"claude-opus-4-5-thinking-32k\"  # The reasoning model\n",
    "baseline_model = \"gpt-5-nano\"  # The baseline (non-reasoning) model\n",
    "\n",
    "# Models list as passed to run_strong_models_experiment.py\n",
    "models = [reasoning_model, baseline_model]\n",
    "print(f\"Models list from config generator: {models}\")\n",
    "print(f\"  models[0] = {models[0]} (this becomes Agent_Alpha)\")\n",
    "print(f\"  models[1] = {models[1]} (this becomes Agent_Beta)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Factory creates:\n",
      "  Agent_Alpha -> claude-opus-4-5-thinking-32k\n",
      "  Agent_Beta -> gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Agent Factory (agent_factory.py lines 55-67)\n",
    "# Creates agents in order of the models list\n",
    "\n",
    "agent_names = [\"Alpha\", \"Beta\"]\n",
    "agent_mapping = {}\n",
    "\n",
    "for i, model_name in enumerate(models):\n",
    "    agent_id = f\"Agent_{agent_names[i]}\"\n",
    "    agent_mapping[agent_id] = model_name\n",
    "    \n",
    "print(\"Agent Factory creates:\")\n",
    "for agent_id, model in agent_mapping.items():\n",
    "    print(f\"  {agent_id} -> {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/DANQIC/jz4391/bargain/.venv/lib/python3.14/site-packages/IPython/core/interactiveshell.py:3073: SyntaxWarning: 'return' in a 'finally' block\n",
      "  return result\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "determine_reasoning_agent_ids() got an unexpected keyword argument 'budget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Test with different model_order values\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_order \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweak_first\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrong_first\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 32\u001b[0m     reasoning_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdetermine_reasoning_agent_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mmodel_order = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_order\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Reasoning prompt assigned to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreasoning_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: determine_reasoning_agent_ids() got an unexpected keyword argument 'budget'"
     ]
    }
   ],
   "source": [
    "# Step 3: Experiment code determines reasoning agent (experiment.py lines 207-218)\n",
    "# This is where the bug occurs!\n",
    "\n",
    "def determine_reasoning_agent_ids(model_order: str, reasoning_budget: int) -> list:\n",
    "    \"\"\"\n",
    "    This is the BUGGY logic from experiment.py lines 207-218.\n",
    "    \n",
    "    The comments say:\n",
    "    - weak_first: models = [baseline, reasoning] -> Agent_Beta is reasoning\n",
    "    - strong_first: models = [reasoning, baseline] -> Agent_Alpha is reasoning\n",
    "    \n",
    "    BUT the config generator ALWAYS passes [reasoning, baseline]!\n",
    "    \"\"\"\n",
    "    if not reasoning_budget:\n",
    "        return []\n",
    "    \n",
    "    # This logic assumes the models list order changes based on model_order\n",
    "    # But it doesn't! The config generator always passes [reasoning, baseline]\n",
    "    if model_order == \"weak_first\":\n",
    "        # Expects: models = [baseline, reasoning]\n",
    "        # Actual:  models = [reasoning, baseline]\n",
    "        reasoning_agent_ids = [\"Agent_Beta\"]  # WRONG! This is the baseline!\n",
    "    else:  # strong_first\n",
    "        # Expects: models = [reasoning, baseline]  \n",
    "        # Actual:  models = [reasoning, baseline]\n",
    "        reasoning_agent_ids = [\"Agent_Alpha\"]  # Correct!\n",
    "    \n",
    "    return reasoning_agent_ids\n",
    "\n",
    "# Test with different model_order values\n",
    "for model_order in [\"weak_first\", \"strong_first\"]:\n",
    "    reasoning_ids = determine_reasoning_agent_ids(model_order, budget=5000)\n",
    "    \n",
    "    print(f\"\\nmodel_order = '{model_order}':\")\n",
    "    print(f\"  Reasoning prompt assigned to: {reasoning_ids}\")\n",
    "    for agent_id in reasoning_ids:\n",
    "        actual_model = agent_mapping[agent_id]\n",
    "        is_reasoning_model = actual_model == reasoning_model\n",
    "        status = \"✓ CORRECT\" if is_reasoning_model else \"✗ BUG! Assigned to baseline!\"\n",
    "        print(f\"  {agent_id} = {actual_model}\")\n",
    "        print(f\"  {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Verify with actual experiment data\n",
    "# Load a run_*_all_interactions.json file to confirm\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Find an experiment with weak_first order\n",
    "experiment_dir = Path(\"../experiments/results/ttc_scaling_20260124_224426\")\n",
    "weak_first_dir = experiment_dir / \"claude-opus-4-5-thinking-32k_vs_gpt-5-nano\" / \"weak_first\" / \"budget_20000\" / \"comp_1_0\"\n",
    "\n",
    "if weak_first_dir.exists():\n",
    "    interactions_file = weak_first_dir / \"run_1_all_interactions.json\"\n",
    "    if interactions_file.exists():\n",
    "        with open(interactions_file) as f:\n",
    "            interactions = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(interactions)} interactions from {interactions_file.name}\")\n",
    "        print(\"\\nSample interactions showing token_usage:\")\n",
    "        \n",
    "        for interaction in interactions[:6]:\n",
    "            agent_id = interaction.get(\"agent_id\")\n",
    "            phase = interaction.get(\"phase\")\n",
    "            token_usage = interaction.get(\"token_usage\", {})\n",
    "            has_reasoning = \"reasoning_tokens\" in token_usage and token_usage[\"reasoning_tokens\"]\n",
    "            \n",
    "            print(f\"\\n  {agent_id} ({phase}):\")\n",
    "            print(f\"    token_usage: {token_usage}\")\n",
    "            print(f\"    has reasoning_tokens: {has_reasoning}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {weak_first_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fix\n",
    "\n",
    "There are two ways to fix this bug:\n",
    "\n",
    "### Option A: Fix in experiment.py (lines 207-218)\n",
    "Change the logic to check the actual model, not assume based on position:\n",
    "\n",
    "```python\n",
    "# Instead of assuming position, check which agent has the reasoning model\n",
    "if reasoning_config.get(\"budget\"):\n",
    "    # Find which agent is the reasoning model by checking model names\n",
    "    reasoning_agent_ids = []\n",
    "    for agent in agents:\n",
    "        # Check if this agent's model is a reasoning model\n",
    "        model_name = agent.config._actual_model_id  # or similar\n",
    "        if is_reasoning_model(model_name):  # Need to implement this check\n",
    "            reasoning_agent_ids.append(agent.agent_id)\n",
    "```\n",
    "\n",
    "### Option B: Fix in config generator (generate_ttc_experiments.py line 191)\n",
    "Pass models in the correct order based on model_order:\n",
    "\n",
    "```python\n",
    "if order == \"weak_first\":\n",
    "    cmd_models = [baseline_model, reasoning_model]  # baseline first\n",
    "else:  # strong_first\n",
    "    cmd_models = [reasoning_model, baseline_model]  # reasoning first\n",
    "\n",
    "cmd = [\n",
    "    'python', 'run_strong_models_experiment.py',\n",
    "    '--models', *cmd_models,\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### Option C: Fix in experiment.py (lines 131-147)\n",
    "Actually reverse the models list when model_order is weak_first:\n",
    "\n",
    "```python\n",
    "if model_order == \"weak_first\":\n",
    "    # Reverse so baseline is first (Agent_Alpha), reasoning is second (Agent_Beta)\n",
    "    models = models[::-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the fix (Option C - reverse models for weak_first)\n",
    "\n",
    "def determine_reasoning_agent_ids_FIXED(model_order: str, models_input: list, reasoning_budget: int) -> tuple:\n",
    "    \"\"\"\n",
    "    FIXED version: Properly handles model ordering.\n",
    "    \n",
    "    Returns (reasoning_agent_ids, actual_models_order)\n",
    "    \"\"\"\n",
    "    if not reasoning_budget:\n",
    "        return [], models_input\n",
    "    \n",
    "    # THE FIX: Actually reverse the models list for weak_first\n",
    "    if model_order == \"weak_first\":\n",
    "        # Config generator passes [reasoning, baseline]\n",
    "        # We need [baseline, reasoning] for weak_first\n",
    "        models = models_input[::-1]  # Reverse!\n",
    "        reasoning_agent_ids = [\"Agent_Beta\"]  # Now correct: Beta = reasoning\n",
    "    else:  # strong_first\n",
    "        models = models_input  # Keep as-is\n",
    "        reasoning_agent_ids = [\"Agent_Alpha\"]  # Correct: Alpha = reasoning\n",
    "    \n",
    "    return reasoning_agent_ids, models\n",
    "\n",
    "# Test the fix\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED VERSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_order in [\"weak_first\", \"strong_first\"]:\n",
    "    reasoning_ids, actual_models = determine_reasoning_agent_ids_FIXED(\n",
    "        model_order, \n",
    "        [reasoning_model, baseline_model],  # Config generator always passes this\n",
    "        budget=5000\n",
    "    )\n",
    "    \n",
    "    # Rebuild agent mapping with the (potentially reversed) models\n",
    "    fixed_agent_mapping = {\n",
    "        f\"Agent_{agent_names[i]}\": actual_models[i]\n",
    "        for i in range(len(actual_models))\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nmodel_order = '{model_order}':\")\n",
    "    print(f\"  Actual models order: {actual_models}\")\n",
    "    print(f\"  Agent mapping:\")\n",
    "    for agent_id, model in fixed_agent_mapping.items():\n",
    "        print(f\"    {agent_id} -> {model}\")\n",
    "    print(f\"  Reasoning prompt assigned to: {reasoning_ids}\")\n",
    "    \n",
    "    for agent_id in reasoning_ids:\n",
    "        actual_model = fixed_agent_mapping[agent_id]\n",
    "        is_reasoning_model = actual_model == reasoning_model\n",
    "        status = \"✓ CORRECT\" if is_reasoning_model else \"✗ STILL WRONG!\"\n",
    "        print(f\"    {agent_id} = {actual_model} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files to Modify\n",
    "\n",
    "The minimal fix should be in **one** of these locations:\n",
    "\n",
    "1. **`strong_models_experiment/experiment.py`** lines 131-147\n",
    "   - Add model reversal for `weak_first`\n",
    "   - This is the cleanest fix since it keeps the rest of the pipeline unchanged\n",
    "\n",
    "2. **`scripts/generate_ttc_experiments.py`** line 191\n",
    "   - Pass models in the correct order based on `model_order`\n",
    "   - Would require regenerating all configs\n",
    "\n",
    "**Recommended**: Fix in `experiment.py` by adding the model reversal logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Test Case: Verify Agent-to-Model Mapping\n",
    "\n",
    "The test below creates agents exactly as the experiment does, then checks which model each agent actually calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal test: Create agents and verify their actual model configuration\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')  # Change to project root\n",
    "\n",
    "from strong_models_experiment.agents import StrongModelAgentFactory\n",
    "from strong_models_experiment.configs import STRONG_MODELS_CONFIG\n",
    "import asyncio\n",
    "\n",
    "async def test_agent_creation():\n",
    "    \"\"\"Test agent creation to verify model mapping.\"\"\"\n",
    "    factory = StrongModelAgentFactory()\n",
    "    \n",
    "    # This is what SLURM passes for strong_first\n",
    "    models = ['claude-opus-4-5-thinking-32k', 'gpt-5-nano']\n",
    "    config = {\n",
    "        'model_order': 'strong_first',\n",
    "        'max_tokens_default': 1000,\n",
    "    }\n",
    "    \n",
    "    print(\"=== INPUT ===\")\n",
    "    print(f\"models list: {models}\")\n",
    "    print(f\"model_order: {config['model_order']}\")\n",
    "    print()\n",
    "    \n",
    "    # Create agents\n",
    "    agents = await factory.create_agents(models, config)\n",
    "    \n",
    "    print(\"=== CREATED AGENTS ===\")\n",
    "    for agent in agents:\n",
    "        print(f\"\\n{agent.agent_id}:\")\n",
    "        print(f\"  type: {type(agent).__name__}\")\n",
    "        print(f\"  model_name: {getattr(agent, 'model_name', 'N/A')}\")\n",
    "        \n",
    "        # Check the actual API configuration\n",
    "        if hasattr(agent, 'config'):\n",
    "            cfg = agent.config\n",
    "            print(f\"  config.model_type: {getattr(cfg, 'model_type', 'N/A')}\")\n",
    "            print(f\"  config._actual_model_id: {getattr(cfg, '_actual_model_id', 'N/A')}\")\n",
    "        \n",
    "        # Check API client\n",
    "        if hasattr(agent, 'client'):\n",
    "            print(f\"  client type: {type(agent.client).__name__}\")\n",
    "\n",
    "# Run the test\n",
    "# await test_agent_creation()  # Uncomment to run\n",
    "\n",
    "print(\"Run the cell above with 'await test_agent_creation()' to test agent creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTUAL ROOT CAUSE (Discovered 2026-01-25)\n",
    "\n",
    "**The bug was NOT about API calls going to wrong endpoints!**\n",
    "\n",
    "### What the token_usage patterns actually mean:\n",
    "\n",
    "1. **Agent_Alpha**: `{\"total_tokens\": 851}` - Anthropic-style response (no nested \"usage\" dict)\n",
    "2. **Agent_Beta**: `{\"input_tokens\": 557, ..., \"reasoning_tokens\": 2048}` - OpenAI-style response (has nested \"usage\" dict)\n",
    "\n",
    "The **format difference** is due to how each agent type returns metadata:\n",
    "- Anthropic agents return `input_tokens`, `output_tokens` directly in metadata (no nested \"usage\")\n",
    "- OpenAI agents return a nested `\"usage\"` dict in metadata\n",
    "\n",
    "### The REAL Bug\n",
    "\n",
    "The `reasoning_token_budget` was being applied to **ALL agents** instead of just the reasoning agent!\n",
    "\n",
    "In `agent_factory.py` lines 72-76 (BEFORE FIX):\n",
    "```python\n",
    "agent = self._create_agent_by_type(\n",
    "    ...\n",
    "    reasoning_token_budget=reasoning_token_budget  # SAME budget for ALL agents!\n",
    ")\n",
    "```\n",
    "\n",
    "This caused:\n",
    "- **Claude (Agent_Alpha)**: Extended thinking enabled, but `thinking_tokens=0` for some reason\n",
    "- **GPT-5-nano (Agent_Beta)**: `reasoning_effort=\"low\"` incorrectly enabled, returns `reasoning_tokens` in response!\n",
    "\n",
    "### The Fix (Implemented)\n",
    "\n",
    "In `agent_factory.py`, only apply `reasoning_token_budget` to the reasoning agent:\n",
    "\n",
    "```python\n",
    "# Determine which agent index should receive the reasoning budget\n",
    "model_order = config.get(\"model_order\", \"weak_first\")\n",
    "if model_order == \"weak_first\":\n",
    "    reasoning_agent_index = 1  # Agent_Beta is reasoning\n",
    "else:  # strong_first\n",
    "    reasoning_agent_index = 0  # Agent_Alpha is reasoning\n",
    "\n",
    "# Only apply to reasoning agent\n",
    "agent_reasoning_budget = reasoning_token_budget if i == reasoning_agent_index else None\n",
    "\n",
    "agent = self._create_agent_by_type(\n",
    "    ...\n",
    "    reasoning_token_budget=agent_reasoning_budget\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the fix: Test agent creation with reasoning budget\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "from strong_models_experiment.agents import StrongModelAgentFactory\n",
    "import asyncio\n",
    "\n",
    "async def verify_reasoning_budget_fix():\n",
    "    \"\"\"Verify that reasoning_token_budget is only applied to the reasoning agent.\"\"\"\n",
    "    factory = StrongModelAgentFactory()\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"strong_first\", ['claude-opus-4-5-thinking-32k', 'gpt-5-nano']),\n",
    "        (\"weak_first\", ['gpt-5-nano', 'claude-opus-4-5-thinking-32k']),\n",
    "    ]\n",
    "    \n",
    "    for model_order, models in test_cases:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {model_order}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        config = {\n",
    "            'model_order': model_order,\n",
    "            'max_tokens_default': 1000,\n",
    "            'reasoning_token_budget': 5000,\n",
    "        }\n",
    "        \n",
    "        agents = await factory.create_agents(models, config)\n",
    "        \n",
    "        # Determine expected reasoning agent\n",
    "        reasoning_agent = \"Agent_Alpha\" if model_order == \"strong_first\" else \"Agent_Beta\"\n",
    "        \n",
    "        print(f\"\\nExpected reasoning agent: {reasoning_agent}\")\n",
    "        print(f\"\\nAgent reasoning configuration:\")\n",
    "        \n",
    "        all_correct = True\n",
    "        for agent in agents:\n",
    "            thinking_budget = agent.config.custom_parameters.get(\"thinking_budget_tokens\", None)\n",
    "            reasoning_effort = agent.config.custom_parameters.get(\"reasoning_effort\", None)\n",
    "            has_reasoning = thinking_budget is not None or reasoning_effort is not None\n",
    "            should_have = agent.agent_id == reasoning_agent\n",
    "            \n",
    "            status = \"✓\" if has_reasoning == should_have else \"✗ BUG!\"\n",
    "            if has_reasoning != should_have:\n",
    "                all_correct = False\n",
    "            \n",
    "            print(f\"  {agent.agent_id} ({type(agent).__name__}):\")\n",
    "            print(f\"    thinking_budget: {thinking_budget}\")\n",
    "            print(f\"    reasoning_effort: {reasoning_effort}\")\n",
    "            print(f\"    has_reasoning={has_reasoning}, should_have={should_have} {status}\")\n",
    "        \n",
    "        if all_correct:\n",
    "            print(f\"\\n✓ Fix verified for {model_order}!\")\n",
    "        else:\n",
    "            print(f\"\\n✗ Bug still present for {model_order}!\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# await verify_reasoning_budget_fix()\n",
    "print(\"Uncomment 'await verify_reasoning_budget_fix()' to test the fix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
