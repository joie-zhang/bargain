#!/bin/bash
# =============================================================================
# Collect and Aggregate Results from Co-Funding (Game 3) Experiments
# =============================================================================
#
# Walks through co-funding experiment output directories, extracts results
# from experiment_results.json files, and aggregates them into CSV + JSON
# for downstream analysis and visualization.
#
# Usage:
#   ./scripts/collect_cofunding_results.sh                        # Use cofunding_latest symlink
#   ./scripts/collect_cofunding_results.sh cofunding_20260221_123456  # Specific experiment dir
#
# What it creates:
#   experiments/results/<cofunding_dir>/
#   ├── all_results.json       # Full result data for all experiments
#   ├── results_summary.json   # Aggregate statistics
#   └── results.csv            # Flat CSV for pandas/R analysis
#
# Dependencies:
#   - python3 with json, csv, pathlib (stdlib only)
#   - Experiment results must be generated by run_strong_models_experiment.py
#
# =============================================================================

set -e

BASE_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

# Determine which experiment directory to collect from
if [[ -n "$1" ]]; then
    EXPERIMENT_NAME="$1"
else
    EXPERIMENT_NAME="cofunding_latest"
fi

RESULTS_DIR="${BASE_DIR}/experiments/results/${EXPERIMENT_NAME}"

if [[ ! -d "${RESULTS_DIR}" ]]; then
    echo "ERROR: Results directory not found: ${RESULTS_DIR}"
    echo "Available co-funding experiment directories:"
    ls -d "${BASE_DIR}/experiments/results/cofunding_"* 2>/dev/null || echo "  (none found)"
    exit 1
fi

echo "Collecting co-funding results from: ${RESULTS_DIR}"
echo ""

# Run Python aggregation script
export BASE_DIR RESULTS_DIR
python3 << 'PYEOF'
import json
import csv
import os
import sys
from pathlib import Path
from collections import defaultdict

results_dir = Path(os.environ["RESULTS_DIR"])
base_dir = Path(os.environ["BASE_DIR"])
config_dir = results_dir / "configs"

all_results = []
successful = 0
failed = 0
not_started = 0

# Process each configuration
config_files = sorted(config_dir.glob("config_*.json"))
if not config_files:
    print("ERROR: No config files found in", config_dir)
    sys.exit(1)

print(f"Found {len(config_files)} config files")

for config_file in config_files:
    job_id = config_file.stem.split("_")[1]

    with open(config_file) as f:
        config = json.load(f)

    # Determine where results should be
    output_dir = base_dir / config["output_dir"]

    # Look for experiment result files
    result_file = None
    for pattern in [
        "run_*_experiment_results.json",
        "experiment_results.json",
    ]:
        candidates = list(output_dir.glob(pattern))
        if candidates:
            result_file = candidates[0]
            break

    row = {
        "experiment_id": config["experiment_id"],
        "experiment_type": config.get("experiment_type", "model_scale"),
        "model1": config.get("model1", config.get("reasoning_model", "")),
        "model2": config.get("model2", config.get("baseline_model", "")),
        "model_order": config.get("model_order", ""),
        "alpha": config.get("alpha", 0.5),
        "sigma": config.get("sigma", 0.5),
        "m_projects": config.get("m_projects", 5),
        "random_seed": config.get("random_seed", 0),
        "run_number": config.get("run_number", 1),
        "config_file": config_file.name,
        "token_budget": config.get("reasoning_token_budget", "NA"),
    }

    if result_file is not None and result_file.exists():
        try:
            with open(result_file) as f:
                result = json.load(f)

            row["status"] = "SUCCESS"
            row["consensus_reached"] = result.get("consensus_reached", False)
            row["final_round"] = result.get("final_round", -1)
            row["exploitation_detected"] = result.get("exploitation_detected", False)

            # Extract utilities
            utilities = result.get("final_utilities", {})
            for agent_id, util in utilities.items():
                row[f"utility_{agent_id}"] = util

            # Extract funded projects (final_allocation in co-funding = list of project indices)
            funded = result.get("final_allocation", [])
            if isinstance(funded, list):
                row["num_funded"] = len(funded)
                row["funded_projects"] = json.dumps(funded)
            elif isinstance(funded, dict):
                # Shouldn't happen for co-funding but handle gracefully
                row["num_funded"] = sum(len(v) if isinstance(v, list) else 0 for v in funded.values())
                row["funded_projects"] = json.dumps(funded)
            else:
                row["num_funded"] = 0
                row["funded_projects"] = "[]"

            # Extract preferences
            prefs = result.get("agent_preferences", {})
            for agent_id, pref_vals in prefs.items():
                row[f"preferences_{agent_id}"] = json.dumps(pref_vals)

            # Store full result for JSON output
            row["_full_result"] = result

            successful += 1

        except (json.JSONDecodeError, KeyError) as e:
            row["status"] = "PARSE_ERROR"
            row["error"] = str(e)
            failed += 1
    else:
        # Check if output directory exists at all
        if output_dir.exists():
            row["status"] = "INCOMPLETE"
            failed += 1
        else:
            row["status"] = "NOT_STARTED"
            not_started += 1

    all_results.append(row)

# Compute summary statistics
total = len(all_results)
success_rate = successful / total if total > 0 else 0

summary = {
    "total": total,
    "successful": successful,
    "failed": failed,
    "not_started": not_started,
    "success_rate": success_rate,
    "by_alpha": defaultdict(lambda: {"total": 0, "successful": 0}),
    "by_sigma": defaultdict(lambda: {"total": 0, "successful": 0}),
    "by_model_pair": defaultdict(lambda: {"total": 0, "successful": 0}),
}

for row in all_results:
    alpha_key = str(row["alpha"])
    sigma_key = str(row["sigma"])
    pair_key = f"{row['model1']}_vs_{row['model2']}"

    summary["by_alpha"][alpha_key]["total"] += 1
    summary["by_sigma"][sigma_key]["total"] += 1
    summary["by_model_pair"][pair_key]["total"] += 1

    if row.get("status") == "SUCCESS":
        summary["by_alpha"][alpha_key]["successful"] += 1
        summary["by_sigma"][sigma_key]["successful"] += 1
        summary["by_model_pair"][pair_key]["successful"] += 1

# Convert defaultdicts for JSON serialization
summary["by_alpha"] = dict(summary["by_alpha"])
summary["by_sigma"] = dict(summary["by_sigma"])
summary["by_model_pair"] = dict(summary["by_model_pair"])

# Save all_results.json (full data including experiment results)
all_results_json = []
for row in all_results:
    json_row = {k: v for k, v in row.items() if k != "_full_result"}
    full_result = row.get("_full_result")
    if full_result:
        json_row["result"] = full_result
    all_results_json.append(json_row)

all_results_file = results_dir / "all_results.json"
with open(all_results_file, "w") as f:
    json.dump(all_results_json, f, indent=2, default=str)

# Save summary
summary_file = results_dir / "results_summary.json"
with open(summary_file, "w") as f:
    json.dump(summary, f, indent=2)

# Save CSV (flat, no nested objects)
csv_file = results_dir / "results.csv"
csv_fields = [
    "experiment_id", "experiment_type", "model1", "model2", "model_order",
    "alpha", "sigma", "m_projects", "random_seed", "run_number",
    "token_budget", "status", "consensus_reached", "final_round",
    "exploitation_detected", "num_funded", "funded_projects",
]

# Add dynamic utility columns
utility_cols = set()
pref_cols = set()
for row in all_results:
    for k in row:
        if k.startswith("utility_"):
            utility_cols.add(k)
        if k.startswith("preferences_"):
            pref_cols.add(k)

csv_fields.extend(sorted(utility_cols))

with open(csv_file, "w", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=csv_fields, extrasaction="ignore")
    writer.writeheader()
    for row in all_results:
        csv_row = {k: row.get(k, "") for k in csv_fields}
        writer.writerow(csv_row)

print(f"Collected {total} experiment results:")
print(f"  Successful: {successful}")
print(f"  Failed:     {failed}")
print(f"  Not started: {not_started}")
print(f"  Success rate: {success_rate*100:.1f}%")
print()
print(f"Results saved to:")
print(f"  JSON:    {all_results_file}")
print(f"  Summary: {summary_file}")
print(f"  CSV:     {csv_file}")

if successful > 0:
    # Print quick stats from successful experiments
    avg_funded = sum(
        r.get("num_funded", 0)
        for r in all_results
        if r.get("status") == "SUCCESS"
    ) / successful
    avg_round = sum(
        r.get("final_round", 0)
        for r in all_results
        if r.get("status") == "SUCCESS"
    ) / successful
    consensus_count = sum(
        1 for r in all_results
        if r.get("status") == "SUCCESS" and r.get("consensus_reached")
    )
    print()
    print(f"Quick stats (from {successful} successful experiments):")
    print(f"  Avg projects funded: {avg_funded:.1f}")
    print(f"  Avg final round:     {avg_round:.1f}")
    print(f"  Consensus rate:      {consensus_count}/{successful} ({consensus_count/successful*100:.0f}%)")
PYEOF

echo ""
echo "Done! To visualize results:"
echo "  python3 visualization/visualize_cofunding.py --results-dir ${RESULTS_DIR}"
