{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Provider Validation Notebook\n",
    "\n",
    "This notebook tests ONE model from EACH API provider to validate your experiment setup before submitting batch jobs.\n",
    "\n",
    "**API Types Tested:**\n",
    "1. **OpenAI** - `gpt-4o-mini` (cheapest)\n",
    "2. **Anthropic** - `claude-3-haiku` (cheapest)\n",
    "3. **Google** - `gemini-2-0-flash-lite` (cheapest)\n",
    "4. **XAI** - `grok-3-mini` (cheapest)\n",
    "5. **OpenRouter** - `amazon-nova-micro` (cheapest)\n",
    "6. **Princeton Cluster** - `Llama-3.2-3B-Instruct` (local HuggingFace)\n",
    "\n",
    "**Run this notebook before batch submissions to catch API issues early!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /scratch/gpfs/DANQIC/jz4391/bargain\n",
      "Working directory: /scratch/gpfs/DANQIC/jz4391/bargain\n"
     ]
    }
   ],
   "source": [
    "# Setup - Add project root to path\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root (parent of tests/)\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == 'tests':\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "API KEY STATUS\n",
      "============================================================\n",
      "✅ OPENAI_API_KEY: SET\n",
      "✅ ANTHROPIC_API_KEY: SET\n",
      "✅ GOOGLE_API_KEY: SET\n",
      "✅ XAI_API_KEY: SET\n",
      "✅ OPENROUTER_API_KEY: SET\n",
      "\n",
      "Note: Princeton Cluster models use local HuggingFace - no API key needed\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "import traceback\n",
    "\n",
    "# Check for required environment variables\n",
    "print(\"=\" * 60)\n",
    "print(\"API KEY STATUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "api_keys = {\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"ANTHROPIC_API_KEY\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"GOOGLE_API_KEY\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    \"XAI_API_KEY\": os.getenv(\"XAI_API_KEY\"),\n",
    "    \"OPENROUTER_API_KEY\": os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "}\n",
    "\n",
    "for key_name, key_value in api_keys.items():\n",
    "    status = \"SET\" if key_value else \"MISSING\"\n",
    "    icon = \"\\u2705\" if key_value else \"\\u274c\"\n",
    "    print(f\"{icon} {key_name}: {status}\")\n",
    "\n",
    "print(\"\\nNote: Princeton Cluster models use local HuggingFace - no API key needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODELS BY API TYPE\n",
      "============================================================\n",
      "\n",
      "ANTHROPIC (9 models):\n",
      "  - claude-4-5-haiku\n",
      "  - claude-4-sonnet\n",
      "  - claude-sonnet-4-5\n",
      "  - claude-4-1-opus\n",
      "  - claude-opus-4-5-thinking-32k\n",
      "  ... and 4 more\n",
      "\n",
      "GOOGLE (7 models):\n",
      "  - gemini-1-5-pro\n",
      "  - gemini-2-5-pro\n",
      "  - gemini-2-0-flash\n",
      "  - gemini-2-0-flash-lite\n",
      "  - gemini-3-pro\n",
      "  ... and 2 more\n",
      "\n",
      "OPENAI (16 models):\n",
      "  - gpt-4o\n",
      "  - gpt-4o-latest\n",
      "  - gpt-4o-mini\n",
      "  - o1\n",
      "  - gpt-5-low-effort\n",
      "  ... and 11 more\n",
      "\n",
      "OPENROUTER (11 models):\n",
      "  - llama-3-1-405b\n",
      "  - glm-4.7\n",
      "  - qwen3-max\n",
      "  - deepseek-r1-0528\n",
      "  - grok-4\n",
      "  ... and 6 more\n",
      "\n",
      "PRINCETON_CLUSTER (18 models):\n",
      "  - gemma-3-27b-it\n",
      "  - QwQ-32B\n",
      "  - llama-3.3-70b-instruct\n",
      "  - Qwen2.5-72B-Instruct\n",
      "  - gemma-2-27b-it\n",
      "  ... and 13 more\n",
      "\n",
      "XAI (4 models):\n",
      "  - grok-4-0709\n",
      "  - grok-3\n",
      "  - grok-3-mini\n",
      "  - grok-4-1-thinking\n"
     ]
    }
   ],
   "source": [
    "# Import project modules\n",
    "from strong_models_experiment.configs import STRONG_MODELS_CONFIG\n",
    "from strong_models_experiment.agents import StrongModelAgentFactory\n",
    "from negotiation.llm_agents import NegotiationContext\n",
    "\n",
    "# Display available models by API type\n",
    "print(\"=\" * 60)\n",
    "print(\"MODELS BY API TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models_by_api = {}\n",
    "for model_name, config in STRONG_MODELS_CONFIG.items():\n",
    "    api_type = config.get(\"api_type\", \"unknown\")\n",
    "    if api_type not in models_by_api:\n",
    "        models_by_api[api_type] = []\n",
    "    # Skip deprecated models\n",
    "    if not config.get(\"deprecated\", False):\n",
    "        models_by_api[api_type].append(model_name)\n",
    "\n",
    "for api_type, models in sorted(models_by_api.items()):\n",
    "    print(f\"\\n{api_type.upper()} ({len(models)} models):\")\n",
    "    for m in models[:5]:  # Show first 5\n",
    "        print(f\"  - {m}\")\n",
    "    if len(models) > 5:\n",
    "        print(f\"  ... and {len(models) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Configuration\n",
    "\n",
    "Select one cheap/fast model from each provider for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test models selected:\n",
      "  openai: gpt-4o-mini\n",
      "  anthropic: claude-3-haiku\n",
      "  google: gemini-2-0-flash-lite\n",
      "  xai: grok-3-mini\n",
      "  openrouter: amazon-nova-micro\n",
      "  princeton_cluster: Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Test models - one from each provider (cheapest/fastest options)\n",
    "TEST_MODELS = {\n",
    "    \"openai\": \"gpt-4o-mini\",\n",
    "    \"anthropic\": \"claude-3-haiku\",\n",
    "    \"google\": \"gemini-2-0-flash-lite\",\n",
    "    \"xai\": \"grok-3-mini\",\n",
    "    \"openrouter\": \"amazon-nova-micro\",\n",
    "    \"princeton_cluster\": \"Llama-3.2-3B-Instruct\",\n",
    "}\n",
    "\n",
    "# Simple test prompt\n",
    "TEST_PROMPT = \"Say 'Hello, I am working!' and nothing else.\"\n",
    "\n",
    "print(\"Test models selected:\")\n",
    "for api_type, model in TEST_MODELS.items():\n",
    "    print(f\"  {api_type}: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results tracking\n",
    "test_results = {}\n",
    "\n",
    "def create_minimal_context() -> NegotiationContext:\n",
    "    \"\"\"\n",
    "    Create a minimal NegotiationContext for testing.\n",
    "    The generate_response method requires this context object.\n",
    "    \"\"\"\n",
    "    return NegotiationContext(\n",
    "        current_round=1,\n",
    "        max_rounds=1,\n",
    "        items=[{\"name\": \"test_item\", \"value\": 1}],\n",
    "        agents=[\"test_agent\"],\n",
    "        agent_id=\"test_agent\",\n",
    "        preferences={\"test_item\": 1},\n",
    "        conversation_history=[],\n",
    "        turn_type=\"discussion\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def test_single_model(model_name: str, api_type: str) -> Tuple[bool, str, float]:\n",
    "    \"\"\"\n",
    "    Test a single model with a simple prompt.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success: bool, message: str, latency: float)\n",
    "    \"\"\"\n",
    "    factory = StrongModelAgentFactory()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create a single agent\n",
    "        config = {\"max_tokens_default\": 100}\n",
    "        agents = await factory.create_agents([model_name], config)\n",
    "        \n",
    "        if not agents:\n",
    "            return False, \"Failed to create agent (check API key)\", 0.0\n",
    "        \n",
    "        agent = agents[0]\n",
    "        \n",
    "        # Create minimal context for testing\n",
    "        context = create_minimal_context()\n",
    "        \n",
    "        # Send test prompt using correct method signature\n",
    "        response = await agent.generate_response(\n",
    "            context=context,\n",
    "            prompt=TEST_PROMPT\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        if response and response.content and len(response.content) > 0:\n",
    "            # Truncate response for display\n",
    "            display_response = response.content[:100] + \"...\" if len(response.content) > 100 else response.content\n",
    "            return True, f\"Response: {display_response}\", latency\n",
    "        else:\n",
    "            return False, \"Empty response received\", latency\n",
    "            \n",
    "    except Exception as e:\n",
    "        latency = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        # Truncate long error messages\n",
    "        if len(error_msg) > 200:\n",
    "            error_msg = error_msg[:200] + \"...\"\n",
    "        return False, f\"Error: {error_msg}\", latency\n",
    "\n",
    "\n",
    "def print_result(api_type: str, model: str, success: bool, message: str, latency: float):\n",
    "    \"\"\"Pretty print test result.\"\"\"\n",
    "    icon = \"\\u2705\" if success else \"\\u274c\"\n",
    "    status = \"PASS\" if success else \"FAIL\"\n",
    "    print(f\"\\n{icon} [{api_type.upper()}] {model}\")\n",
    "    print(f\"   Status: {status}\")\n",
    "    print(f\"   Latency: {latency:.2f}s\")\n",
    "    print(f\"   {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Test OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OPENAI with model: gpt-4o-mini\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ [OPENAI] gpt-4o-mini\n",
      "   Status: PASS\n",
      "   Latency: 1.79s\n",
      "   Response: Hello, I am working!\n"
     ]
    }
   ],
   "source": [
    "api_type = \"openai\"\n",
    "model = TEST_MODELS[api_type]\n",
    "\n",
    "print(f\"Testing {api_type.upper()} with model: {model}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"\\u274c OPENAI_API_KEY not set - skipping\")\n",
    "    test_results[api_type] = {\"success\": False, \"message\": \"API key missing\", \"latency\": 0}\n",
    "else:\n",
    "    success, message, latency = await test_single_model(model, api_type)\n",
    "    test_results[api_type] = {\"success\": success, \"message\": message, \"latency\": latency, \"model\": model}\n",
    "    print_result(api_type, model, success, message, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Test Anthropic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ANTHROPIC with model: claude-3-haiku\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ [ANTHROPIC] claude-3-haiku\n",
      "   Status: PASS\n",
      "   Latency: 1.09s\n",
      "   Response: Hello, I am working!\n"
     ]
    }
   ],
   "source": [
    "api_type = \"anthropic\"\n",
    "model = TEST_MODELS[api_type]\n",
    "\n",
    "print(f\"Testing {api_type.upper()} with model: {model}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    print(\"\\u274c ANTHROPIC_API_KEY not set - skipping\")\n",
    "    test_results[api_type] = {\"success\": False, \"message\": \"API key missing\", \"latency\": 0}\n",
    "else:\n",
    "    success, message, latency = await test_single_model(model, api_type)\n",
    "    test_results[api_type] = {\"success\": success, \"message\": message, \"latency\": latency, \"model\": model}\n",
    "    print_result(api_type, model, success, message, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Test Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GOOGLE with model: gemini-2-0-flash-lite\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ [GOOGLE] gemini-2-0-flash-lite\n",
      "   Status: PASS\n",
      "   Latency: 1.63s\n",
      "   Response: Hello, I am working!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api_type = \"google\"\n",
    "model = TEST_MODELS[api_type]\n",
    "\n",
    "print(f\"Testing {api_type.upper()} with model: {model}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"\\u274c GOOGLE_API_KEY not set - skipping\")\n",
    "    test_results[api_type] = {\"success\": False, \"message\": \"API key missing\", \"latency\": 0}\n",
    "else:\n",
    "    success, message, latency = await test_single_model(model, api_type)\n",
    "    test_results[api_type] = {\"success\": success, \"message\": message, \"latency\": latency, \"model\": model}\n",
    "    print_result(api_type, model, success, message, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Test XAI (Grok) API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing XAI with model: grok-3-mini\n",
      "--------------------------------------------------\n",
      "❌ XAI_API_KEY not set - skipping\n"
     ]
    }
   ],
   "source": [
    "api_type = \"xai\"\n",
    "model = TEST_MODELS[api_type]\n",
    "\n",
    "print(f\"Testing {api_type.upper()} with model: {model}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if not os.getenv(\"XAI_API_KEY\"):\n",
    "    print(\"\\u274c XAI_API_KEY not set - skipping\")\n",
    "    test_results[api_type] = {\"success\": False, \"message\": \"API key missing\", \"latency\": 0}\n",
    "else:\n",
    "    success, message, latency = await test_single_model(model, api_type)\n",
    "    test_results[api_type] = {\"success\": success, \"message\": message, \"latency\": latency, \"model\": model}\n",
    "    print_result(api_type, model, success, message, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test OpenRouter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OPENROUTER with model: amazon-nova-micro\n",
      "--------------------------------------------------\n",
      "[client] Wrote request_1768990049-056579.json\n",
      "[client] Got response for 1768990049-056579: success\n",
      "\n",
      "✅ [OPENROUTER] amazon-nova-micro\n",
      "   Status: PASS\n",
      "   Latency: 1.09s\n",
      "   Response: Hello, I am working! Let's make sure this negotiation is as efficient and mutually beneficial as pos...\n"
     ]
    }
   ],
   "source": [
    "api_type = \"openrouter\"\n",
    "model = TEST_MODELS[api_type]\n",
    "\n",
    "print(f\"Testing {api_type.upper()} with model: {model}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if not os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"\\u274c OPENROUTER_API_KEY not set - skipping\")\n",
    "    test_results[api_type] = {\"success\": False, \"message\": \"API key missing\", \"latency\": 0}\n",
    "else:\n",
    "    success, message, latency = await test_single_model(model, api_type)\n",
    "    test_results[api_type] = {\"success\": success, \"message\": message, \"latency\": latency, \"model\": model}\n",
    "    print_result(api_type, model, success, message, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Test Princeton Cluster (Local HuggingFace)\n",
    "\n",
    "**Note:** This requires:\n",
    "1. Running on a cluster node with GPU access (e.g., della-gpu)\n",
    "2. Model weights available at `/scratch/gpfs/DANQIC/models/`\n",
    "3. `transformers` and `torch` packages installed\n",
    "\n",
    "If not on cluster, this test will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PRINCETON_CLUSTER with model: Llama-3.2-3B-Instruct\n",
      "--------------------------------------------------\n",
      "   GPU detected: NVIDIA A100-PCIE-40GB\n",
      "   Model path: /scratch/gpfs/DANQIC/models/Llama-3.2-3B-Instruct\n",
      "   Loading model (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ [PRINCETON_CLUSTER] Llama-3.2-3B-Instruct\n",
      "   Status: PASS\n",
      "   Latency: 37.70s\n",
      "   Response: Hello, I am working!\n"
     ]
    }
   ],
   "source": [
    "api_type = \"princeton_cluster\"\n",
    "model = TEST_MODELS[api_type]\n",
    "\n",
    "print(f\"Testing {api_type.upper()} with model: {model}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if we're on the cluster by looking for model directory\n",
    "model_config = STRONG_MODELS_CONFIG.get(model, {})\n",
    "local_path = model_config.get(\"local_path\", \"\")\n",
    "models_base = \"/scratch/gpfs/DANQIC/models\"\n",
    "\n",
    "if not os.path.exists(models_base):\n",
    "    print(\"\\u26a0\\ufe0f Not on Princeton cluster - skipping local model test\")\n",
    "    print(f\"   (Models directory not found: {models_base})\")\n",
    "    print(\"   Run this notebook on della-gpu or similar to test local models\")\n",
    "    test_results[api_type] = {\"success\": None, \"message\": \"Not on cluster\", \"latency\": 0}\n",
    "elif local_path and not os.path.exists(local_path):\n",
    "    print(f\"\\u274c Model path not found: {local_path}\")\n",
    "    print(\"   Available models in /scratch/gpfs/DANQIC/models/:\")\n",
    "    try:\n",
    "        for m in sorted(os.listdir(models_base))[:10]:\n",
    "            print(f\"     - {m}\")\n",
    "    except:\n",
    "        pass\n",
    "    test_results[api_type] = {\"success\": False, \"message\": f\"Model path missing: {local_path}\", \"latency\": 0}\n",
    "else:\n",
    "    # Check for GPU availability\n",
    "    try:\n",
    "        import torch\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        if gpu_available:\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            print(f\"   GPU detected: {gpu_name}\")\n",
    "        else:\n",
    "            print(\"   \\u26a0\\ufe0f No GPU detected - model loading will be slow\")\n",
    "    except ImportError:\n",
    "        print(\"   \\u26a0\\ufe0f torch not available\")\n",
    "    \n",
    "    print(f\"   Model path: {local_path}\")\n",
    "    print(\"   Loading model (this may take a while)...\")\n",
    "    \n",
    "    success, message, latency = await test_single_model(model, api_type)\n",
    "    test_results[api_type] = {\"success\": success, \"message\": message, \"latency\": latency, \"model\": model}\n",
    "    print_result(api_type, model, success, message, latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Timestamp: 2026-01-21T05:08:23.748997\n",
      "\n",
      "✅ OPENAI               | PASS |   1.79s | gpt-4o-mini\n",
      "✅ ANTHROPIC            | PASS |   1.09s | claude-3-haiku\n",
      "✅ GOOGLE               | PASS |   1.63s | gemini-2-0-flash-lite\n",
      "❌ XAI                  | FAIL |   0.00s | grok-3-mini\n",
      "✅ OPENROUTER           | PASS |   1.09s | amazon-nova-micro\n",
      "✅ PRINCETON_CLUSTER    | PASS |  37.70s | Llama-3.2-3B-Instruct\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total: 6 | Passed: 5 | Failed: 1 | Skipped: 0\n",
      "\n",
      "⚠️ Some tests FAILED - fix issues before running batch experiments!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTimestamp: {datetime.now().isoformat()}\")\n",
    "print()\n",
    "\n",
    "passed = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "\n",
    "for api_type, result in test_results.items():\n",
    "    success = result.get(\"success\")\n",
    "    model = result.get(\"model\", TEST_MODELS.get(api_type, \"unknown\"))\n",
    "    latency = result.get(\"latency\", 0)\n",
    "    message = result.get(\"message\", \"\")\n",
    "    \n",
    "    if success is True:\n",
    "        icon = \"\\u2705\"\n",
    "        status = \"PASS\"\n",
    "        passed += 1\n",
    "    elif success is False:\n",
    "        icon = \"\\u274c\"\n",
    "        status = \"FAIL\"\n",
    "        failed += 1\n",
    "    else:\n",
    "        icon = \"\\u26a0\\ufe0f\"\n",
    "        status = \"SKIP\"\n",
    "        skipped += 1\n",
    "    \n",
    "    print(f\"{icon} {api_type.upper():20} | {status:4} | {latency:6.2f}s | {model}\")\n",
    "\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "total = passed + failed + skipped\n",
    "print(f\"Total: {total} | Passed: {passed} | Failed: {failed} | Skipped: {skipped}\")\n",
    "\n",
    "if failed > 0:\n",
    "    print(\"\\n\\u26a0\\ufe0f Some tests FAILED - fix issues before running batch experiments!\")\n",
    "elif passed == total - skipped and passed > 0:\n",
    "    print(\"\\n\\u2705 All tested APIs are working! Safe to submit batch jobs.\")\n",
    "else:\n",
    "    print(\"\\n\\u26a0\\ufe0f Some tests were skipped - verify those APIs separately if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (Optional) Run a Mini Negotiation Experiment\n",
    "\n",
    "This runs a full 2-round negotiation with 2 items to test the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to run a mini experiment\n",
    "RUN_MINI_EXPERIMENT = False\n",
    "\n",
    "# Choose which API to test (use one that passed above)\n",
    "MINI_EXPERIMENT_API = \"anthropic\"  # Change as needed\n",
    "MINI_EXPERIMENT_MODEL = TEST_MODELS.get(MINI_EXPERIMENT_API, \"claude-3-haiku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini experiment skipped. Set RUN_MINI_EXPERIMENT = True to run.\n"
     ]
    }
   ],
   "source": [
    "if RUN_MINI_EXPERIMENT:\n",
    "    from strong_models_experiment import StrongModelsExperiment\n",
    "    import tempfile\n",
    "    \n",
    "    print(f\"Running mini experiment with {MINI_EXPERIMENT_MODEL}...\")\n",
    "    print(\"Configuration: 2 agents, 2 items, 2 rounds\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Use temporary output directory\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        experiment = StrongModelsExperiment(output_dir=tmp_dir)\n",
    "        \n",
    "        # Run a minimal experiment\n",
    "        config = {\n",
    "            \"m_items\": 2,\n",
    "            \"t_rounds\": 2,\n",
    "            \"competition_level\": 0.5,\n",
    "            \"random_seed\": 42,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            start = time.time()\n",
    "            results = await experiment.run_single_experiment(\n",
    "                models=[MINI_EXPERIMENT_MODEL, MINI_EXPERIMENT_MODEL],\n",
    "                experiment_config=config\n",
    "            )\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            print(f\"\\n\\u2705 Mini experiment completed in {elapsed:.1f}s\")\n",
    "            print(f\"   Consensus reached: {results.consensus_reached}\")\n",
    "            print(f\"   Final round: {results.final_round}\")\n",
    "            print(f\"   Final utilities: {results.final_utilities}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n\\u274c Mini experiment failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"Mini experiment skipped. Set RUN_MINI_EXPERIMENT = True to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: /scratch/gpfs/DANQIC/jz4391/bargain/tests/provider_test_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON for CI/automation\n",
    "output_path = project_root / \"tests\" / \"provider_test_results.json\"\n",
    "\n",
    "export_data = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"summary\": {\n",
    "        \"passed\": passed,\n",
    "        \"failed\": failed,\n",
    "        \"skipped\": skipped,\n",
    "    },\n",
    "    \"results\": test_results,\n",
    "}\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **API Key Missing**: Set the environment variable before starting Jupyter:\n",
    "   ```bash\n",
    "   export OPENAI_API_KEY=\"sk-...\"\n",
    "   export ANTHROPIC_API_KEY=\"sk-ant-...\"\n",
    "   # etc.\n",
    "   ```\n",
    "\n",
    "2. **Princeton Cluster Model Not Found**: \n",
    "   - Verify the model exists: `ls /scratch/gpfs/DANQIC/models/`\n",
    "   - Check `STRONG_MODELS_CONFIG` for the correct `local_path`\n",
    "\n",
    "3. **Rate Limiting**: If you get rate limit errors, wait a minute and retry.\n",
    "\n",
    "4. **Timeout**: Some models (especially local ones) may take longer. The default timeout is 30s.\n",
    "\n",
    "5. **Import Errors**: Ensure you're in the project virtual environment:\n",
    "   ```bash\n",
    "   source .venv/bin/activate\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
