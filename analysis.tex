In \Cref{sec:scaling_laws}, we present results for the baseline model's utility when negotiating against adversarial agents spanning the full capability spectrum, using Chatbot Arena Elo scores as our capability metric. \Cref{sec:negotiation_rounds_analysis} analyzes how negotiation dynamics vary with opponent capability and competition level.

\section{Scaling Laws for Strategic Interactions}
\label{sec:scaling_laws}

% How does baseline model performance (expected utility) change as a function of opponent capability and competition level?

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Figures/jan_23_results/gpt5_nano_utility_vs_elo.png}
    \caption{\textbf{Baseline Model Utility vs.\ Adversary Elo Rating.} Each point represents the mean payoff earned by the baseline model (GPT-5-nano, Elo $\approx 1338$) when negotiating against a given adversary, aggregated across all competition levels and orderings. Points are colored by adversary tier (Strong: Elo $\geq 1415$; Medium: $1290 \leq$ Elo $< 1415$; Weak: Elo $< 1290$). The dashed trend line reveals a weak and non-monotonic relationship: the baseline does not suffer a clean, monotone decline in utility as adversary capability increases.}
    \label{fig:utility_vs_elo}
\end{figure}

We evaluate GPT-5-nano (Elo $\approx 1338$) as a fixed baseline agent against 35 opponents drawn from three tiers spanning an Elo range of 1112 to 1490. All 35 matchups achieved a \textbf{100\% consensus rate}: every negotiation terminated in a mutually agreed-upon allocation. The baseline's mean utility across all matchups is $68.3\%$, with a range of $45.1\%$ (vs.\ Llama-3.1-8B) to $78.6\%$ (vs.\ DeepSeek-R1).

\paragraph{Non-Monotonic Relationship Between Capability and Exploitation.} Contrary to the intuition that stronger adversaries exploit weaker baselines more aggressively, we find only a weak negative correlation between adversary Elo and the baseline's utility (see \Cref{fig:utility_vs_elo}). Several medium- and weak-tier models produce lower baseline utilities than many strong-tier models. For example, Claude-3-Haiku (Elo 1262) yields the worst mean utility to the baseline at $47.4\%$—worse than any strong-tier model, the best of which (DeepSeek-R1-0528, Elo 1418) grants the baseline $75.9\%$. This suggests that raw capability as measured by Elo does not straightforwardly predict negotiation exploitability.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Figures/jan_23_results/gpt5_nano_utility_by_opponent.png}
    \caption{\textbf{Baseline Payoff Distribution by Adversary Model.} Box plots of GPT-5-nano's utility across all runs against each adversary, sorted by decreasing Elo and colored by tier. Individual run outcomes are overlaid as scatter points. The dashed horizontal line at 50 denotes an equal split. The baseline consistently exceeds the equal-split threshold against most opponents, including several strong-tier models.}
    \label{fig:utility_by_opponent}
\end{figure}

\paragraph{Reasoning Models Are More Cooperative.} A consistent pattern emerges when separating adversaries by model type. \Cref{fig:reasoning_effect} shows that reasoning models (DeepSeek-R1, DeepSeek-R1-0528, GPT-5.2-High, Claude-Opus-4.5-Thinking-32K, O3-Mini-High) yield a mean baseline utility of approximately $73.5\%$, whereas non-reasoning models yield approximately $66.8\%$. This gap is not explained by Elo alone: reasoning-capable models at similar Elo ratings consistently produce more balanced allocations. One possible explanation is that reasoning models, given more computational budget, converge rapidly to efficient and fair outcomes rather than engaging in prolonged positional bargaining.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{Figures/jan_23_results/gpt5_nano_vs_reasoning.png}
    \caption{\textbf{Baseline Utility Against Reasoning vs.\ Non-Reasoning Adversaries.} GPT-5-nano earns substantially higher mean utility when negotiating against reasoning-capable models ($\approx 73.5\%$) compared to non-reasoning models ($\approx 66.8\%$), despite reasoning models having higher average Elo ratings.}
    \label{fig:reasoning_effect}
\end{figure}

\paragraph{Effect of Competition Level.} \Cref{fig:competition_effects} shows the baseline's utility and total joint efficiency as a function of competition level $\gamma \in \{0.0, 0.25, 0.5, 0.75, 1.0\}$. At $\gamma = 0.0$, agents share cooperative preferences and most negotiations conclude in one to two rounds with high efficiency. As competition increases, the baseline's utility decreases and joint efficiency declines, reflecting a growing tension between individual incentives and collective welfare.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Figures/jan_23_results/gpt5_nano_competition_effects.png}
    \caption{\textbf{Baseline Utility and Joint Efficiency vs.\ Competition Level.} Left: Mean baseline utility as a function of competition level $\gamma$, averaged across all adversaries. Right: Total joint utility (sum of both agents' payoffs) by competition level, with the theoretical maximum of 200 shown as a dashed reference. Both metrics decline as competition rises, indicating that agents leave more surplus on the table in high-competition settings.}
    \label{fig:competition_effects}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Figures/jan_23_results/gpt5_nano_utility_heatmap.png}
    \caption{\textbf{Baseline Utility Heatmap by Adversary and Competition Level.} Each cell shows the mean payoff earned by GPT-5-nano against a given adversary (rows, sorted by decreasing Elo) at a given competition level (columns). Green indicates higher utility; red indicates lower utility. The heatmap reveals substantial heterogeneity that is not captured by either dimension alone.}
    \label{fig:utility_heatmap}
\end{figure}

\section{Negotiation Rounds Analysis}
\label{sec:negotiation_rounds_analysis}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Figures/jan_23_results/gpt5_nano_rounds_analysis.png}
    \caption{\textbf{Negotiation Length by Adversary Tier.} Left: Distribution of rounds to consensus across all experiments. Right: Mean rounds to consensus broken down by adversary tier (Strong, Medium, Weak), with 95\% confidence intervals. Strong-tier models converge significantly faster than medium- or weak-tier models.}
    \label{fig:rounds_analysis}
\end{figure}

\paragraph{Negotiation Length vs.\ Adversary Capability.} \Cref{fig:rounds_analysis} reveals a striking inverse relationship between adversary capability and negotiation length. Strong-tier models reach consensus in an average of $1$–$2$ rounds—with QwQ-32B (1.59), DeepSeek-R1 (1.15), and Qwen3-Max (1.05) converging particularly fast—while several medium- and weak-tier models require far more rounds. Claude-3-Haiku averages $6.95$ rounds, and GPT-3.5-Turbo averages $5.91$. This pattern holds despite 100\% consensus being reached in all cases, and suggests that less capable models are less efficient proposal generators: they make more incremental offers before landing on mutually acceptable terms.

\paragraph{Negotiation Length vs.\ Competition Level.} As competition level increases from 0.0 to 1.0, the number of rounds required to reach consensus increases substantially. At $\gamma = 0.0$, most negotiations conclude in one to two rounds. At $\gamma = 1.0$, negotiations regularly extend to the maximum allowed rounds, consistent with agents holding out longer to maximize their own share. This is reflected in the declining consensus efficiency observed in \Cref{fig:competition_effects}.
